{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain + Gaugid Integration\n",
        "\n",
        "This notebook demonstrates how to integrate Gaugid SDK with LangChain to build a personalized chatbot.\n",
        "\n",
        "## Setup\n",
        "\n",
        "First, install dependencies and set up environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies (run once)\n",
        "# !uv pip install -e ../../docs[langchain]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import asyncio\n",
        "from typing import List\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from gaugid import GaugidClient\n",
        "\n",
        "# Set your API keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-key-here\"  # Replace with your key\n",
        "os.environ[\"GAUGID_CONNECTION_TOKEN\"] = \"your-token-here\"  # Replace with your token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gaugid Memory Class\n",
        "\n",
        "Create a LangChain-compatible memory class that integrates with Gaugid profiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GaugidMemory:\n",
        "    \"\"\"LangChain-compatible memory that integrates with Gaugid profiles.\"\"\"\n",
        "    \n",
        "    def __init__(self, client: GaugidClient, user_did: str, agent_did: str):\n",
        "        self.client = client\n",
        "        self.user_did = user_did\n",
        "        self.agent_did = agent_did\n",
        "        self.conversation_memory = ConversationBufferMemory(\n",
        "            return_messages=True,\n",
        "            memory_key=\"chat_history\"\n",
        "        )\n",
        "        self.user_context = \"\"\n",
        "    \n",
        "    async def load_user_context(self) -> str:\n",
        "        \"\"\"Load user context from Gaugid profile.\"\"\"\n",
        "        profile = await self.client.get_profile(\n",
        "            user_did=self.user_did,\n",
        "            scopes=[\"a2p:preferences\", \"a2p:professional\", \"a2p:context\", \"a2p:interests\"]\n",
        "        )\n",
        "        \n",
        "        # Extract memories and format as context\n",
        "        memories = profile.get(\"memories\", {}).get(\"episodic\", [])\n",
        "        context_parts = []\n",
        "        \n",
        "        for memory in memories:\n",
        "            category = memory.get(\"category\", \"general\")\n",
        "            content = memory.get(\"content\", \"\")\n",
        "            context_parts.append(f\"- [{category}] {content}\")\n",
        "        \n",
        "        self.user_context = \"\\n\".join(context_parts) if context_parts else \"No user context available.\"\n",
        "        return self.user_context\n",
        "    \n",
        "    async def propose_memory(self, content: str, category: str, confidence: float = 0.7) -> None:\n",
        "        \"\"\"Propose a learned memory back to the user's profile.\"\"\"\n",
        "        await self.client.propose_memory(\n",
        "            user_did=self.user_did,\n",
        "            content=content,\n",
        "            category=category,\n",
        "            confidence=confidence,\n",
        "            context=\"Learned during LangChain conversation\"\n",
        "        )\n",
        "    \n",
        "    def add_user_message(self, message: str) -> None:\n",
        "        \"\"\"Add a user message to conversation memory.\"\"\"\n",
        "        self.conversation_memory.chat_memory.add_user_message(message)\n",
        "    \n",
        "    def add_ai_message(self, message: str) -> None:\n",
        "        \"\"\"Add an AI message to conversation memory.\"\"\"\n",
        "        self.conversation_memory.chat_memory.add_ai_message(message)\n",
        "    \n",
        "    def get_chat_history(self) -> List[BaseMessage]:\n",
        "        \"\"\"Get the chat history.\"\"\"\n",
        "        return self.conversation_memory.chat_memory.messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Personalized Chain\n",
        "\n",
        "Create a LangChain chain with personalized system prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_personalized_chain(llm: ChatOpenAI, user_context: str):\n",
        "    \"\"\"Create a LangChain chain with personalized system prompt.\"\"\"\n",
        "    \n",
        "    system_template = f\"\"\"You are a helpful AI assistant specializing in software development.\n",
        "\n",
        "## USER PROFILE (from Gaugid)\n",
        "{user_context}\n",
        "\n",
        "## Guidelines\n",
        "- Adapt your communication style to match user preferences\n",
        "- Reference their expertise level when explaining concepts\n",
        "- Consider their current projects and learning goals\n",
        "- Be aware of their tool preferences\n",
        "- Personalize examples and recommendations\n",
        "\n",
        "Be helpful, technical, and precise. Adapt to the user's expertise level.\"\"\"\n",
        "    \n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", system_template),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ])\n",
        "    \n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    return chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize and Load User Context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Gaugid client\n",
        "client = GaugidClient(connection_token=os.environ[\"GAUGID_CONNECTION_TOKEN\"])\n",
        "user_did = \"did:a2p:user:demo\"\n",
        "\n",
        "# Create memory with Gaugid integration\n",
        "memory = GaugidMemory(\n",
        "    client=client,\n",
        "    user_did=user_did,\n",
        "    agent_did=\"did:a2p:agent:langchain-assistant\"\n",
        ")\n",
        "\n",
        "# Load user context\n",
        "user_context = await memory.load_user_context()\n",
        "print(\"User context loaded:\")\n",
        "print(user_context)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Chain and Chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create personalized chain\n",
        "llm = ChatOpenAI(model=\"gpt-5.2\", temperature=0.7)\n",
        "chain = create_personalized_chain(llm, user_context)\n",
        "\n",
        "# Example conversation\n",
        "user_input = \"What are the best practices for async Python?\"\n",
        "\n",
        "# Add to memory and get response\n",
        "memory.add_user_message(user_input)\n",
        "\n",
        "response = chain.invoke({\n",
        "    \"input\": user_input,\n",
        "    \"chat_history\": memory.get_chat_history()[:-1],\n",
        "})\n",
        "\n",
        "memory.add_ai_message(response)\n",
        "print(f\"User: {user_input}\")\n",
        "print(f\"\\nAssistant: {response}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Propose Memory\n",
        "\n",
        "After learning something new, propose it to the user's profile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Propose a learned memory\n",
        "await memory.propose_memory(\n",
        "    content=\"User is exploring LangChain for AI applications\",\n",
        "    category=\"a2p:interests.technology\",\n",
        "    confidence=0.75\n",
        ")\n",
        "print(\"âœ… Memory proposed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "\n",
        "Always close the client when done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Close the client\n",
        "await client.close()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
