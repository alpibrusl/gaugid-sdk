{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LlamaIndex + Gaugid Memory Block Integration\n",
        "\n",
        "This notebook demonstrates how to use `GaugidMemoryBlock` with LlamaIndex's Memory system, allowing agents to store and retrieve long-term memories using Gaugid profiles.\n",
        "\n",
        "**Installation**: `pip install gaugid[llama-index]`\n",
        "\n",
        "Based on: https://github.com/run-llama/llama_index/blob/main/llama-index-core/llama_index/core/memory/memory.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import asyncio\n",
        "\n",
        "try:\n",
        "    from llama_index.core.memory import Memory\n",
        "    from llama_index.core.base.llms.types import ChatMessage\n",
        "    from llama_index.llms.openai import OpenAI\n",
        "    from gaugid.integrations.llama_index import GaugidMemoryBlock\n",
        "    LLAMA_INDEX_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LLAMA_INDEX_AVAILABLE = False\n",
        "    print(\"⚠️  LlamaIndex not installed. Install with: pip install gaugid[llama-index]\")\n",
        "\n",
        "# Set your API keys\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"your-key-here\"\n",
        "os.environ[\"GAUGID_CONNECTION_TOKEN\"] = \"your-token-here\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Gaugid memory blocks\n",
        "print(\"1️⃣ Creating GaugidMemoryBlocks...\")\n",
        "\n",
        "# User preferences block (semantic memory)\n",
        "preferences_block = GaugidMemoryBlock(\n",
        "    name=\"user_preferences\",\n",
        "    connection_token=os.getenv(\"GAUGID_CONNECTION_TOKEN\"),\n",
        "    description=\"User preferences and settings\",\n",
        "    priority=1,  # High priority - don't truncate\n",
        "    memory_type=\"semantic\",\n",
        ")\n",
        "\n",
        "# Knowledge block (semantic memory)\n",
        "knowledge_block = GaugidMemoryBlock(\n",
        "    name=\"user_knowledge\",\n",
        "    connection_token=os.getenv(\"GAUGID_CONNECTION_TOKEN\"),\n",
        "    description=\"User's knowledge and facts\",\n",
        "    priority=2,\n",
        "    memory_type=\"semantic\",\n",
        ")\n",
        "\n",
        "# Context block (episodic memory)\n",
        "context_block = GaugidMemoryBlock(\n",
        "    name=\"conversation_context\",\n",
        "    connection_token=os.getenv(\"GAUGID_CONNECTION_TOKEN\"),\n",
        "    description=\"Conversation context and history\",\n",
        "    priority=3,  # Lower priority - can be truncated\n",
        "    memory_type=\"episodic\",\n",
        ")\n",
        "\n",
        "print(\"   ✅ Created 3 memory blocks:\")\n",
        "print(\"      - user_preferences (semantic)\")\n",
        "print(\"      - user_knowledge (semantic)\")\n",
        "print(\"      - conversation_context (episodic)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LlamaIndex Memory with Gaugid blocks\n",
        "print(\"2️⃣ Creating LlamaIndex Memory with Gaugid blocks...\")\n",
        "memory = Memory(\n",
        "    memory_blocks=[\n",
        "        preferences_block,\n",
        "        knowledge_block,\n",
        "        context_block,\n",
        "    ],\n",
        "    session_id=\"user-123\",\n",
        "    token_limit=30000,\n",
        ")\n",
        "print(\"   ✅ Memory created\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create LLM\n",
        "llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), model=\"gpt-4o-mini\")\n",
        "\n",
        "# Add messages to memory\n",
        "print(\"3️⃣ Adding messages to memory...\")\n",
        "user_message = ChatMessage(\n",
        "    role=\"user\",\n",
        "    content=\"I prefer dark mode for all applications.\"\n",
        ")\n",
        "await memory.aput(user_message)\n",
        "print(f\"   ✅ Added: {user_message.content}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add another message\n",
        "user_message2 = ChatMessage(\n",
        "    role=\"user\",\n",
        "    content=\"I'm working on a Python project using FastAPI.\"\n",
        ")\n",
        "await memory.aput(user_message2)\n",
        "print(f\"   ✅ Added: {user_message2.content}\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
